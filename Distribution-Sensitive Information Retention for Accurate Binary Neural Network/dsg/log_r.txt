Invalid GPU ID: 0<=1
|===>Creating data loader
self._set_dataloader() success!
self._set_model() success!
save epoch: 00
self._change_paddle() success!
self._replace() success!
ResNet(
  (conv1): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
  (bn1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)
  (relu): Sequential(
    (0): ReLU()
    (1): QuantAct(activation_bit=8, full_precision_flag=False, running_stat=True, Act_min: 0.00, Act_max: 0.00)
  )
  (maxpool): MaxPool2D(kernel_size=3, stride=2, padding=1)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)
      (relu): Sequential(
        (0): ReLU()
        (1): QuantAct(activation_bit=8, full_precision_flag=False, running_stat=True, Act_min: 0.00, Act_max: 0.00)
      )
      (conv2): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn2): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)
    )
    (1): BasicBlock(
      (conv1): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)
      (relu): Sequential(
        (0): ReLU()
        (1): QuantAct(activation_bit=8, full_precision_flag=False, running_stat=True, Act_min: 0.00, Act_max: 0.00)
      )
      (conv2): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn2): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)
      (relu): Sequential(
        (0): ReLU()
        (1): QuantAct(activation_bit=8, full_precision_flag=False, running_stat=True, Act_min: 0.00, Act_max: 0.00)
      )
      (conv2): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn2): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)
      (downsample): Sequential(
        (0): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
        (1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)
      )
    )
    (1): BasicBlock(
      (conv1): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)
      (relu): Sequential(
        (0): ReLU()
        (1): QuantAct(activation_bit=8, full_precision_flag=False, running_stat=True, Act_min: 0.00, Act_max: 0.00)
      )
      (conv2): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn2): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
      (relu): Sequential(
        (0): ReLU()
        (1): QuantAct(activation_bit=8, full_precision_flag=False, running_stat=True, Act_min: 0.00, Act_max: 0.00)
      )
      (conv2): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
      (downsample): Sequential(
        (0): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
        (1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
      )
    )
    (1): BasicBlock(
      (conv1): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
      (relu): Sequential(
        (0): ReLU()
        (1): QuantAct(activation_bit=8, full_precision_flag=False, running_stat=True, Act_min: 0.00, Act_max: 0.00)
      )
      (conv2): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
      (relu): Sequential(
        (0): ReLU()
        (1): QuantAct(activation_bit=8, full_precision_flag=False, running_stat=True, Act_min: 0.00, Act_max: 0.00)
      )
      (conv2): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
      (downsample): Sequential(
        (0): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
        (1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
      )
    )
    (1): BasicBlock(
      (conv1): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
      (relu): Sequential(
        (0): ReLU()
        (1): QuantAct(activation_bit=8, full_precision_flag=False, running_stat=True, Act_min: 0.00, Act_max: 0.00)
      )
      (conv2): (Quant_Conv2d() weight_bit=8, full_precision_flag=False)
      (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)
    )
  )
  (avgpool): AdaptiveAvgPool2D(output_size=(1, 1))
  (fc): (Quant_Linear() weight_bit=8, full_precision_flag=False)
)
self._set_trainer() success!

 self.unfreeze_model(self.model)
[Epoch 1/10]
 self.unfreeze_model(self.model)
[Epoch 2/10]
 self.unfreeze_model(self.model)
[Epoch 3/10]
 self.unfreeze_model(self.model)
[Epoch 4/10]
[Epoch 5/10]
[Epoch 6/10][acc: 69.6540%]
[Epoch 7/10][acc: 69.6540%]
[Epoch 8/10][acc: 69.6480%]
[Epoch 9/10][acc: 69.6480%]
[Epoch 10/10][acc: 69.6500%]

Model: ResNet18
Parameters(M): 11.69 -> 2.92
Acc: 71.0 -> 69.7
Running Time is: 0:26:04.502569
